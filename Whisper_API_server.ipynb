{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Google Colab Whisper API Server - GPU Optimized Version\n",
    "# This version properly utilizes T4 GPU and handles tensor errors\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Disable logging completely\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "logging.getLogger('werkzeug').setLevel(logging.CRITICAL)\n",
    "os.environ['WERKZEUG_RUN_MAIN'] = 'true'\n",
    "\n",
    "# Check GPU availability first\n",
    "print(\"🔍 Checking GPU availability...\")\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"✅ GPU detected: {gpu_name}\")\n",
    "        print(f\"🎯 CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"💾 GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "        DEVICE = \"cuda\"\n",
    "    else:\n",
    "        print(\"⚠️ No GPU detected, using CPU\")\n",
    "        DEVICE = \"cpu\"\n",
    "except ImportError:\n",
    "    print(\"📦 PyTorch not found, will install...\")\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "# Install packages with GPU support\n",
    "print(\"📦 Installing required packages with GPU support...\")\n",
    "try:\n",
    "    import subprocess\n",
    "\n",
    "    # Install PyTorch with CUDA support first\n",
    "    if DEVICE == \"cuda\" or torch.cuda.is_available():\n",
    "        print(\"🚀 Installing PyTorch with CUDA support...\")\n",
    "        result = subprocess.run([\n",
    "            sys.executable, '-m', 'pip', 'install',\n",
    "            'torch', 'torchaudio', '--index-url', 'https://download.pytorch.org/whl/cu118'\n",
    "        ], capture_output=True, text=True, timeout=300)\n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ PyTorch with CUDA installed\")\n",
    "        else:\n",
    "            print(\"⚠️ CUDA PyTorch installation had issues, using CPU version\")\n",
    "\n",
    "    # Install other packages\n",
    "    result = subprocess.run([\n",
    "        sys.executable, '-m', 'pip', 'install',\n",
    "        'flask', 'flask-cors', 'openai-whisper', 'pyngrok', 'python-multipart',\n",
    "        'requests', 'ffmpeg-python', 'numpy>=1.21.0'\n",
    "    ], capture_output=True, text=True, timeout=300)\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        print(f\"⚠️ Package installation had warnings: {result.stderr}\")\n",
    "    else:\n",
    "        print(\"✅ Packages installed successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Package installation failed: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "import time\n",
    "import tempfile\n",
    "import socket\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import threading\n",
    "import json\n",
    "from werkzeug.serving import make_server\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from flask import Flask, request, jsonify\n",
    "    from flask_cors import CORS\n",
    "    import whisper\n",
    "    import torch\n",
    "    from pyngrok import ngrok\n",
    "    import requests\n",
    "    import ffmpeg\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import failed: {e}\")\n",
    "    print(\"🔄 Retrying package installation...\")\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', '--force-reinstall', 'flask', 'flask-cors'])\n",
    "    from flask import Flask, request, jsonify\n",
    "    from flask_cors import CORS\n",
    "\n",
    "# Re-check GPU after PyTorch installation\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = \"cuda\"\n",
    "        torch.cuda.empty_cache()  # Clear GPU cache\n",
    "        print(f\"🎯 Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        DEVICE = \"cpu\"\n",
    "        print(\"📱 Using CPU\")\n",
    "except:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"📱 Using CPU (fallback)\")\n",
    "\n",
    "# Constants\n",
    "MAX_FILE_SIZE = 300 * 1024 * 1024  # 100MB\n",
    "DEFAULT_MAX_LENGTH = 42\n",
    "DEFAULT_MAX_LINES = 2\n",
    "VALID_AUDIO_EXTENSIONS = ['.mp3', '.wav', '.flac', '.m4a', '.ogg', '.webm', '.mp4', '.avi', '.mov']\n",
    "\n",
    "# Configuration - Set your static domain here\n",
    "STATIC_DOMAIN = \"terrier-hip-sunbeam.ngrok-free.app\"  # Replace with your actual static domain\n",
    "NGROK_AUTH_TOKEN = \"6mQy4iw5BXtaV2BJQe7vR_3JBuZQKpB1pxweCMv92tQ\"  # Your ngrok auth token\n",
    "\n",
    "# Global variables\n",
    "whisper_models = {}\n",
    "server_instance = None\n",
    "\n",
    "def find_free_port():\n",
    "    \"\"\"Find a free port to use\"\"\"\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            s.bind(('', 0))\n",
    "            s.listen(1)\n",
    "            port = s.getsockname()[1]\n",
    "        return port\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error finding free port: {e}\")\n",
    "        return 5000  # fallback port\n",
    "\n",
    "def create_app():\n",
    "    \"\"\"Create Flask app with minimal logging\"\"\"\n",
    "    app = Flask(__name__)\n",
    "    CORS(app)\n",
    "\n",
    "    # Disable Flask logging\n",
    "    app.logger.disabled = True\n",
    "    app.logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "    return app\n",
    "\n",
    "def load_whisper_model(model_name=\"turbo\"):\n",
    "    \"\"\"Load and cache whisper model with GPU support\"\"\"\n",
    "    try:\n",
    "        if model_name not in whisper_models:\n",
    "            print(f\"📥 Loading Whisper model: {model_name} on {DEVICE}\")\n",
    "\n",
    "            # Clear GPU cache before loading\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Load model with device specification\n",
    "            model = whisper.load_model(model_name, device=DEVICE)\n",
    "            whisper_models[model_name] = model\n",
    "\n",
    "            if DEVICE == \"cuda\":\n",
    "                print(f\"🎯 Model loaded on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "                print(f\"💾 GPU memory used: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "            else:\n",
    "                print(f\"📱 Model loaded on CPU\")\n",
    "\n",
    "        return whisper_models[model_name]\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model {model_name}: {e}\")\n",
    "        # Fallback to CPU if GPU fails\n",
    "        if DEVICE == \"cuda\":\n",
    "            print(\"🔄 Retrying on CPU...\")\n",
    "            try:\n",
    "                model = whisper.load_model(model_name, device=\"cpu\")\n",
    "                whisper_models[model_name] = model\n",
    "                return model\n",
    "            except Exception as e2:\n",
    "                print(f\"❌ CPU fallback also failed: {e2}\")\n",
    "        raise\n",
    "\n",
    "def is_valid_audio_file(filename):\n",
    "    \"\"\"Check if file has valid audio extension\"\"\"\n",
    "    ext = Path(filename).suffix.lower()\n",
    "    return ext in VALID_AUDIO_EXTENSIONS\n",
    "\n",
    "def preprocess_audio(file_path):\n",
    "    \"\"\"Preprocess audio to handle potential issues\"\"\"\n",
    "    try:\n",
    "        # Check if file exists and has content\n",
    "        if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:\n",
    "            raise ValueError(\"Audio file is empty or doesn't exist\")\n",
    "\n",
    "        # Use ffmpeg to convert to a standard format\n",
    "        output_path = file_path + \"_processed.wav\"\n",
    "\n",
    "        try:\n",
    "            (\n",
    "                ffmpeg\n",
    "                .input(file_path)\n",
    "                .output(output_path, acodec='pcm_s16le', ac=1, ar='16000')\n",
    "                .overwrite_output()\n",
    "                .run(capture_stdout=True, capture_stderr=True)\n",
    "            )\n",
    "\n",
    "            # Check if processed file was created and has content\n",
    "            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
    "                return output_path\n",
    "            else:\n",
    "                print(\"⚠️ Processed file is empty, using original\")\n",
    "                return file_path\n",
    "\n",
    "        except ffmpeg.Error as e:\n",
    "            print(f\"⚠️ FFmpeg processing failed: {e}, using original file\")\n",
    "            return file_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Audio preprocessing failed: {e}\")\n",
    "        return file_path\n",
    "\n",
    "def convert_to_srt_time(seconds):\n",
    "    \"\"\"Convert seconds to SRT time format (HH:MM:SS,mmm)\"\"\"\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = seconds % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}:{secs:06.3f}\".replace('.', ',')\n",
    "\n",
    "def split_text_for_srt(text, max_length, max_lines):\n",
    "    \"\"\"Split text into lines for SRT format\"\"\"\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    current_line = []\n",
    "\n",
    "    for word in words:\n",
    "        test_line = ' '.join(current_line + [word])\n",
    "        if len(test_line) > max_length and current_line:\n",
    "            lines.append(' '.join(current_line))\n",
    "            current_line = [word]\n",
    "\n",
    "            if len(lines) >= max_lines:\n",
    "                remaining_words = words[words.index(word):]\n",
    "                if lines:\n",
    "                    lines[-1] += ' ' + ' '.join(remaining_words)\n",
    "                break\n",
    "        else:\n",
    "            current_line.append(word)\n",
    "\n",
    "    if current_line and len(lines) < max_lines:\n",
    "        lines.append(' '.join(current_line))\n",
    "\n",
    "    return lines[:max_lines]\n",
    "\n",
    "def generate_srt(segments, max_length, max_lines):\n",
    "    \"\"\"Generate SRT content from segments\"\"\"\n",
    "    srt_content = []\n",
    "\n",
    "    for i, segment in enumerate(segments, 1):\n",
    "        lines = split_text_for_srt(segment['text'], max_length, max_lines)\n",
    "\n",
    "        srt_content.append(str(i))\n",
    "        srt_content.append(f\"{convert_to_srt_time(segment['start'])} --> {convert_to_srt_time(segment['end'])}\")\n",
    "        srt_content.extend(lines)\n",
    "        srt_content.append(\"\")\n",
    "\n",
    "    return '\\n'.join(srt_content).strip()\n",
    "\n",
    "# Create Flask app\n",
    "app = create_app()\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    gpu_info = {}\n",
    "    if DEVICE == \"cuda\":\n",
    "        try:\n",
    "            gpu_info = {\n",
    "                'gpu_name': torch.cuda.get_device_name(0),\n",
    "                'gpu_memory_allocated': f\"{torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\",\n",
    "                'gpu_memory_total': f\"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\"\n",
    "            }\n",
    "        except:\n",
    "            gpu_info = {'gpu_error': 'Could not get GPU info'}\n",
    "\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'version': '2.0.0-gpu',\n",
    "        'device': DEVICE,\n",
    "        'models_loaded': list(whisper_models.keys()),\n",
    "        **gpu_info\n",
    "    })\n",
    "\n",
    "@app.route('/models', methods=['GET'])\n",
    "def list_models():\n",
    "    available_models = ['tiny', 'base', 'small', 'medium', 'large','turbo']\n",
    "    return jsonify({\n",
    "        'models': [f'ggml-{model}.bin' for model in available_models],\n",
    "        'default': 'ggml-medium.bin',\n",
    "        'loaded': list(whisper_models.keys()),\n",
    "        'device': DEVICE,\n",
    "        'recommended': 'ggml-turbo.bin' if DEVICE == \"cuda\" else 'ggml-medium.bin'\n",
    "    })\n",
    "\n",
    "@app.route('/transcribe', methods=['POST'])\n",
    "def transcribe_audio():\n",
    "    processed_file = None\n",
    "    try:\n",
    "        if 'audio' not in request.files:\n",
    "            return jsonify({\n",
    "                'error': 'missing_audio_file',\n",
    "                'message': \"No audio file provided in 'audio' field\"\n",
    "            }), 400\n",
    "\n",
    "        file = request.files['audio']\n",
    "        if file.filename == '':\n",
    "            return jsonify({\n",
    "                'error': 'missing_audio_file',\n",
    "                'message': \"No audio file provided in 'audio' field\"\n",
    "            }), 400\n",
    "\n",
    "        if not is_valid_audio_file(file.filename):\n",
    "            return jsonify({\n",
    "                'error': 'invalid_file_type',\n",
    "                'message': 'Supported formats: mp3, wav, flac, m4a, ogg, webm, mp4, avi, mov'\n",
    "            }), 400\n",
    "\n",
    "        # Parse parameters\n",
    "        language = request.form.get('language', '')\n",
    "        model = request.form.get('model', 'ggml-turbo.bin')\n",
    "        output_srt = request.form.get('output_srt', '').lower() in ['true', '1']\n",
    "        max_length = int(request.form.get('max_length', DEFAULT_MAX_LENGTH))\n",
    "        max_lines = int(request.form.get('max_lines', DEFAULT_MAX_LINES))\n",
    "\n",
    "        # Extract model name\n",
    "        model_name = model.replace('ggml-', '').replace('.bin', '')\n",
    "        if model_name not in ['tiny', 'base', 'small', 'medium', 'large','turbo']:\n",
    "            model_name = 'turbo'\n",
    "\n",
    "        # Use turbo model on GPU for better accuracy\n",
    "        if DEVICE == \"cuda\" and model_name == 'turbo':\n",
    "            print(\"🎯 Using turbo model on GPU for better accuracy\")\n",
    "            model_name = 'turbo'\n",
    "\n",
    "        # Save temp file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as temp_file:\n",
    "            file.save(temp_file.name)\n",
    "            temp_path = temp_file.name\n",
    "\n",
    "        try:\n",
    "            # Preprocess audio to handle potential issues\n",
    "            print(f\"🔧 Preprocessing audio file: {file.filename}\")\n",
    "            processed_file = preprocess_audio(temp_path)\n",
    "\n",
    "            # Load model and transcribe\n",
    "            print(f\"🎯 Processing {file.filename} with model {model_name} on {DEVICE}\")\n",
    "            model_obj = load_whisper_model(model_name)\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Transcription options with better error handling\n",
    "            transcribe_options = {\n",
    "                'fp16': DEVICE == \"cuda\",  # Use FP16 on GPU for speed\n",
    "                'verbose': False,  # Reduce output\n",
    "            }\n",
    "\n",
    "            if language:\n",
    "                transcribe_options['language'] = language\n",
    "\n",
    "            # Clear GPU cache before transcription\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            result = model_obj.transcribe(processed_file, **transcribe_options)\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            print(f\"✅ Transcription completed in {duration:.2f}s on {DEVICE}\")\n",
    "\n",
    "            # Clear GPU cache after transcription\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Validate result\n",
    "            if not result or 'text' not in result:\n",
    "                raise ValueError(\"Transcription returned empty result\")\n",
    "\n",
    "            # Prepare response\n",
    "            response = {\n",
    "                'text': result['text'].strip(),\n",
    "                'language': result.get('language', language),\n",
    "                'duration': duration,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'filename': file.filename,\n",
    "                'format': 'srt' if output_srt else 'text',\n",
    "                'device': DEVICE,\n",
    "                'model': model_name\n",
    "            }\n",
    "\n",
    "            if output_srt and 'segments' in result:\n",
    "                srt_content = generate_srt(result['segments'], max_length, max_lines)\n",
    "                response['srt'] = srt_content\n",
    "\n",
    "            return jsonify(response)\n",
    "\n",
    "        finally:\n",
    "            # Cleanup temp files\n",
    "            for path in [temp_path, processed_file]:\n",
    "                if path and os.path.exists(path):\n",
    "                    try:\n",
    "                        os.unlink(path)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Transcription error: {e}\")\n",
    "\n",
    "        # Clear GPU cache on error\n",
    "        if DEVICE == \"cuda\":\n",
    "            try:\n",
    "                torch.cuda.empty_cache()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return jsonify({\n",
    "            'error': 'transcription_failed',\n",
    "            'message': str(e),\n",
    "            'device': DEVICE\n",
    "        }), 500\n",
    "\n",
    "def test_flask_locally(port):\n",
    "    \"\"\"Test if Flask is running locally\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"http://localhost:{port}/health\", timeout=2)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def start_server():\n",
    "    \"\"\"Start the Flask server with better error handling\"\"\"\n",
    "    global server_instance\n",
    "    port = find_free_port()\n",
    "    print(f\"🚀 Starting server on port {port}\")\n",
    "\n",
    "    try:\n",
    "        # Create server instance\n",
    "        server_instance = make_server('0.0.0.0', port, app, threaded=True)\n",
    "\n",
    "        # Start server in thread\n",
    "        def run_server():\n",
    "            try:\n",
    "                print(f\"🔧 Flask server starting on 0.0.0.0:{port}\")\n",
    "                server_instance.serve_forever()\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Server error: {e}\")\n",
    "\n",
    "        server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "        server_thread.start()\n",
    "\n",
    "        # Wait for server to be ready\n",
    "        print(\"⏳ Waiting for Flask to start...\")\n",
    "        max_attempts = 20\n",
    "        for attempt in range(max_attempts):\n",
    "            if test_flask_locally(port):\n",
    "                print(\"✅ Flask server is ready!\")\n",
    "                break\n",
    "            time.sleep(0.5)\n",
    "            if attempt == max_attempts - 1:\n",
    "                print(\"❌ Flask server failed to start\")\n",
    "                return None, None, port\n",
    "\n",
    "        # Setup ngrok tunnel with static domain\n",
    "        try:\n",
    "            print(\"🌐 Setting up ngrok tunnel with static domain...\")\n",
    "\n",
    "            # Set ngrok auth token\n",
    "            ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "            # Create tunnel with static domain\n",
    "            if STATIC_DOMAIN:\n",
    "                print(f\"🔗 Using static domain: {STATIC_DOMAIN}\")\n",
    "                tunnel = ngrok.connect(port, hostname=STATIC_DOMAIN)\n",
    "                url = f\"https://{STATIC_DOMAIN}\"\n",
    "            else:\n",
    "                print(\"🔗 Using dynamic domain\")\n",
    "                tunnel = ngrok.connect(port)\n",
    "                url = tunnel.public_url\n",
    "\n",
    "            print(f\"🌐 Public URL: {url}\")\n",
    "\n",
    "            # Test public endpoint\n",
    "            print(\"🧪 Testing public endpoint...\")\n",
    "            time.sleep(3)  # Give ngrok time to establish tunnel\n",
    "\n",
    "            try:\n",
    "                response = requests.get(f\"{url}/health\", timeout=15)\n",
    "                if response.status_code == 200:\n",
    "                    print(\"✅ Public endpoint is working!\")\n",
    "                    return tunnel, url, port\n",
    "                else:\n",
    "                    print(f\"⚠️ Public endpoint returned: {response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Public endpoint test failed: {e}\")\n",
    "                print(\"🔧 This might be normal for static domains - they may take longer to propagate\")\n",
    "\n",
    "            return tunnel, url, port\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error setting up ngrok: {e}\")\n",
    "            print(\"🔧 Server is still running locally for testing\")\n",
    "            return None, f\"http://localhost:{port}\", port\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error starting server: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to start the server\"\"\"\n",
    "    print(\"🎙️ Setting up GPU-Optimized Whisper API Server...\")\n",
    "    print(f\"🎯 Device: {DEVICE}\")\n",
    "\n",
    "    if STATIC_DOMAIN:\n",
    "        print(f\"🔗 Static domain configured: {STATIC_DOMAIN}\")\n",
    "    else:\n",
    "        print(\"⚠️ No static domain configured - will use dynamic domain\")\n",
    "\n",
    "    # Pre-load model to test installation\n",
    "    try:\n",
    "        print(\"🔍 Testing Whisper installation...\")\n",
    "        # Use smaller model for initial test\n",
    "        test_model = \"turbo\" if DEVICE == \"cuda\" else \"tiny\"\n",
    "        load_whisper_model(test_model)\n",
    "        print(\"✅ Whisper is working correctly\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Whisper test failed: {e}\")\n",
    "        return\n",
    "\n",
    "    tunnel, public_url, port = start_server()\n",
    "\n",
    "    if public_url:\n",
    "        print(f\"\\n🎉 Server is running!\")\n",
    "        print(f\"🔗 URL: {public_url}\")\n",
    "        print(f\"🔗 Health check: {public_url}/health\")\n",
    "        print(f\"🔗 Transcribe: {public_url}/transcribe\")\n",
    "        print(f\"🔗 Models: {public_url}/models\")\n",
    "        print(f\"🎯 Device: {DEVICE}\")\n",
    "\n",
    "        if DEVICE == \"cuda\":\n",
    "            print(f\"🚀 GPU acceleration enabled!\")\n",
    "            print(f\"💡 Recommended model: turbo (better accuracy)\")\n",
    "        else:\n",
    "            print(f\"📱 CPU mode - consider smaller models for speed\")\n",
    "\n",
    "        print(f\"\\n📋 Example usage:\")\n",
    "        print(f'curl -X POST \"{public_url}/transcribe\" \\\\')\n",
    "        print('  -F \"audio=@your_file.mp3\" \\\\')\n",
    "        print('  -F \"output_srt=true\" \\\\')\n",
    "        print('  -F \"language=en\" \\\\')\n",
    "        print('  -F \"model=ggml-turbo.bin\"')\n",
    "\n",
    "        print(\"\\n📝 Server is ready to accept audio files!\")\n",
    "        print(\"⏹️ The server will run continuously. Interrupt the kernel to stop.\")\n",
    "\n",
    "        try:\n",
    "            # Keep running with periodic GPU memory cleanup\n",
    "            while True:\n",
    "                time.sleep(30)\n",
    "\n",
    "                # Clear GPU cache periodically\n",
    "                if DEVICE == \"cuda\":\n",
    "                    try:\n",
    "                        torch.cuda.empty_cache()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                # Health check\n",
    "                try:\n",
    "                    if tunnel and not test_flask_locally(port):\n",
    "                        print(\"⚠️ Local server seems to have stopped\")\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n🛑 Stopping server...\")\n",
    "            try:\n",
    "                if tunnel:\n",
    "                    ngrok.disconnect(tunnel)\n",
    "                    ngrok.kill()\n",
    "                if server_instance:\n",
    "                    server_instance.shutdown()\n",
    "\n",
    "                # Final GPU cleanup\n",
    "                if DEVICE == \"cuda\":\n",
    "                    torch.cuda.empty_cache()\n",
    "            except:\n",
    "                pass\n",
    "            print(\"✅ Server stopped\")\n",
    "    else:\n",
    "        print(\"❌ Failed to start server\")\n",
    "        print(\"🔧 Check the error messages above for troubleshooting\")\n",
    "\n",
    "# Run the server\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "else:\n",
    "    # If running in notebook, start automatically\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
